***** Can you write some observations of why model like KNN fails to produce good results unlike RandomForest and MultinomialNB? *****

- As Machine learning algorithms does not work on Text data directly, we need to convert them into numeric vector and feed that into models while training.
- In this process, we convert text into a very high dimensional numeric vector using the technique of Bag of words.
- Model like K-Nearest Neighbours(KNN) doesn't work well with high dimensional data because with large number of dimensions, it becomes difficult for the algorithm to calculate distance in each dimension. In higher dimensional space, the cost to calculate distance becomes expensive and hence impacts the performance of model.
- The easy calculation of probabilities for the words in corpus(Bag of words) and storing them in contigency table is the major reason for the Multinomial NaiveBayes to be a text classification friendly algorithm.
- As Random Forest uses Bootstrapping(Row and column Sampling) with many decision tree and overcomes the high variance and overfitting of high dimensional data and also uses feature importance of words for better classifing the categories.
- Machine Learning is like trial and error scientific method, where we keep trying all the possible algorithms we have and select the one which give good results and satisfy the requirements like latency, interpretability etc.

------------------------------------------
Refer these resources to get good idea:

https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/
https://analyticsindiamag.com/naive-bayes-why-is-it-favoured-for-text-related-tasks/